# Linear regression with multiple variables
Wiki: [Wiki](https://share.coursera.org/wiki/index.php/ML:Linear_Regression_with_Multiple_Variables)

## General
* Goal: we try to predict something, based on more than one variable / feature
* Features: xâ‚, xâ‚‚, xâ‚ƒ, ...
* m: amount of examples
* n: amount of features (variables) per example
* xâ±: input (features) of the i-th training example
* xâ‚‚Â¹: value of the 2-nd feature of the first training example
* xÂ²: n-dimensional vector the second (training example)
* Hypothesis: hÎ¸(x) = Î¸â‚€ * xâ‚€ + Î¸â‚ * xâ‚ + Î¸â‚‚ * xâ‚‚ + Î¸â‚ƒ * xâ‚ƒ + ...
* We've added xâ‚€ = 1 as the multiplicator for linear factor (for convenience of notation)
* Hypothesis expressed with vectors: hÎ¸(x) = Î¸T*x (the transposed Î¸-vector multiplied with the
  x-vector)

## Gradient descent for multiple variables
* Hypothesis: hÎ¸(x) = Î¸T*x
* Parameters: Î¸ (vector)
* Cost function: J(Î¸) = (1/2m) * âˆ‘1..m(hÎ¸(xâ±) - yâ±)Â²
* Gradient descent general: Repeat:**Î¸â±¼ := Î¸â±¼ - ð›¼ * (d/dÎ¸â±¼) * J(Î¸)** (simultaneously update for
  every j = 0, ..., n)
* Gradient descent concrete: Repeat:**Î¸â±¼ := Î¸â±¼ - ð›¼ * (1/m) * âˆ‘i=1..m((hÎ¸(xâ±) - yâ±) * xâ±¼â±)** (j is the
  index counting the features, i is the index counting the examples for the sum)

## Practical tricks
### Feature Scaling and normalisation
* Make sure that all features are on a similar scale => quicker conversion
* We try to get every feature in the range -1 <= xáµ¢ <= 1 (1/3 or 3 are still okish, but rather
  not more)
* Formula:**xáµ¢ = (xáµ¢ - Î¼áµ¢) / sáµ¢**
* Î¼ is the average value of xáµ¢ among all examples -> feature scaling
* s is the range xáµ¢ among all examples (max value - min value) -> normalisation

### Learning rate
* Make sure that gradient descent is working correctly: plot the value of the cost function J(Î¸)
  after each iteration. J(Î¸) should decrease with each iteration
* The plot can also help us judge whether or not the algorithm has converged yet
* Rule of thumb: declare convergence if the value of J(Î¸) decreases by less than 10â»Â³ in one
  iteration
* Plot helps to see errors in gradient descent
* If ð›¼ is too big, so the error becomes bigger instead of smaller with every iteration
* If ð›¼ is too small: slow convergence
* To choose ð›¼, try 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 (each step about 3x the previous value)

## Features and polynomial regression
* Features might be combined. E.g. xâ‚ = frontage, xâ‚‚ = depth. We decide to just use a new feature
  x = xâ‚ * xâ‚‚ (size/area = frontage * depth)
* Polynomial regression: Instead of linear functions for fitting, we can use polynomials. We just
  add more parameters Î¸ to the hypothesis and use the powered version of the feature as new
  features:**hÎ¸(x) = Î¸â‚€ + Î¸â‚ * size + Î¸â‚‚ * sizeÂ² + Î¸â‚ƒ * sizeÂ³** (important to apply feature
  scaling here, because the powers make the features a lot bigger)
* We can also use other functions like the square root of a feature, etc. (depending on the data)


# Computing parameters analytically
## Normal equation
* Method for solving for Î¸ analytically
* If Î¸ is a number: J(Î¸) = a * Î¸Â² + b * Î¸ + c => Derivative of the function needs to be zero at the
  local minimum
* If Î¸ is a vector: Same principle, but with all the partial derivatives set to 0
* How to do this: pack all the features values (columns) of all the examples (rows) in a matrix *X*
  (set 1 for xâ‚€). You get a m * (n + 1) matrix. Pack all y-values in a y-vector of dimension m
* X is called the**design matrix**
* Solution is:**Î¸ = (XT * X)â»Â¹ * XT * y**
* Octave: pinv(X' * X) * X' * y
* For small sets of features, this is appropriate and simple (because no iterations, no need to
  choose ð›¼). For large sets of features (n > 1000 or 10'000), it becomes really slow, because
  calculating the inverse matrix has a complexity of O(nÂ³)
* Noninvertibility:
    * Some matrices can't be inverted - mathematically troublesome, but Octave takes
      care of this (as long as we use the pseudo-inverse-function*pinv*instead of*inv*)
    * Reasons:
        * Redundant features (linearly dependent), e.g. the size in feet and in mÂ³
        * Too many features or not enough training data (n >= m). In this case we can delete some
          features or us regularisation (will be covered later)


 # Octave
 Octave Documentation: [Octave Documentation](https://www.gnu.org/software/octave/doc/interpreter/)
 Octave Tutorial: [Octave Tutorial](https://share.coursera.org/wiki/index.php/ML:Octave_Tutorial)


##  Quiz:
### Question 1
      x = (x - avg) / range
     
      x = 94
      avg: (89 + 72 + 94 + 69) / 4 = 324 / 4 = 81
      range: 94 - 69 = 25
     
      x = (94 - 81) / 25 = 0.52


##  Programming assignment:
### How to calculate the cost function
* J = 1/(2 * m) * sum((X * theta - y).^2)

### Gradient descent as matrix operations
* As we need to multiply the elements of each row of "X * theta - y" with their respective column
  in X (the "inner loop" is for the examples), we just multiply everything with the transpose of X.
  This takes care of building the sum at the same time
* theta = theta - alpha * (1/m) * X' * (X * theta - y)
